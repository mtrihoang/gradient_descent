[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/mtrihoang/gradient_descent/main.svg)](https://results.pre-commit.ci/latest/github/mtrihoang/gradient_descent/main)
[![image](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

# Examples of Gradient Descent

The repository shows some applications of Gradient Descent algorithms in finding
local/global optimum in simple settings. The below figures illustrate the speed of
convergence to local/global optimum of objective functions by changing values of
learning rate and decay factor.

Example 1: Estimate regression coefficients.
![GD_with_momentum_1](./GD_without_momentum.gif)

Example 2: Find the optimum of function `y = sin(x) + cos(x/2)`.
![GD_with_momentum_2](./GD_with_momentum_2.gif)
![GD_with_momentum_1](./GD_with_momentum_1.gif)
